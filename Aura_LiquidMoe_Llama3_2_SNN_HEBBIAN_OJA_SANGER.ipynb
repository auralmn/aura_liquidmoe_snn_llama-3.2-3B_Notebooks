{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1EaRNSajynPMZN22RAtLh53agkMfY2eIp",
      "authorship_tag": "ABX9TyNIkFT2woZdimCME6GEVrDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auralmn/aura_liquidmoe_snn_llama-3.2-3B_Notebooks/blob/main/Aura_LiquidMoe_Llama3_2_SNN_HEBBIAN_OJA_SANGER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHUUirjJHFC"
      },
      "outputs": [],
      "source": [
        "# AURA LIQUID MOE (AKA TEST 4.0 - AMYGDALA EXPERTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AURA LIQUIDMOE SNN + LLAMA 3.2 3B UNSLOTH LLM (L4-T4)\n"
      ],
      "metadata": {
        "id": "a1unRLWYJfZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install dependencies"
      ],
      "metadata": {
        "id": "BX6it6TaJqmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip uninstall wandb\n",
        "!pip install transformers torch sentence-transformers accelerate numpy\n",
        "!pip install huggingface_hub\n",
        "# 1. Install Unsloth\n",
        "!pip install \"unsloth[colab-new]\"\n",
        "\n",
        "!pip install huggingface_hub datasets\n",
        "\n",
        "\n",
        "# 3. Authenticate (from Step 13)\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful!\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab or 'HF_TOKEN' secret not found.\")\n",
        "# --- Log into Hugging Face ---\n",
        "# This is the best way to do it in Colab.\n",
        "# 1. Go to your Colab notebook.\n",
        "# 2. Click the \"Key\" icon (Secrets) in the left-hand sidebar.\n",
        "# 3. Create a new secret named \"HF_TOKEN\".\n",
        "# 4. Paste your Hugging Face Pro token (it starts with \"hf_\") as the value.\n",
        "# 5. Make sure \"Notebook access\" is toggled ON.\n",
        "\n",
        "# This code will then automatically find and use your secret.\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful!\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab or 'HF_TOKEN' secret not found.\")\n",
        "    # You can paste your token manually if prompted\n",
        "    # login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_f9MqN2TKQz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP CLEAN AURA INSTANCE"
      ],
      "metadata": {
        "id": "2kGteWhnKuZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum"
      ],
      "metadata": {
        "id": "EHsp-1lMT8oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- All Imports ---\n",
        "from enum import Enum\n",
        "import random\n",
        "import uuid\n",
        "import enum\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import itertools\n",
        "import torch\n",
        "from transformers import AutoTokenizer, logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "from typing import List, Dict, Optional, Union, Any, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from abc import ABC, abstractmethod\n",
        "from numpy import ndarray\n",
        "import asyncio\n",
        "import io\n",
        "import csv\n",
        "import threading # For Memory Pool\n",
        "import time\n",
        "from datasets import load_dataset # <-- NEW: For GoEmotions\n",
        "\n",
        "# --- PyTorch Imports (for Pre-Training) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Suppress transformer warnings\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# --- 0. Colab/HF Login ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"Hugging Face login successful!\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"HF_TOKEN secret not found or not in Colab. Ensure you are logged in.\")\n",
        "\n",
        "# --- 1. Memory Pool (from memory_pool.py) ---\n",
        "@dataclass\n",
        "class PoolStats:\n",
        "    hits: int = 0; misses: int = 0; total_allocations: int = 0; peak_usage_mb: float = 0.0\n",
        "class ArrayPool:\n",
        "    \"\"\"High-performance array pool\"\"\"\n",
        "    def __init__(self, max_pool_size_mb: int = 512):\n",
        "        self.max_pool_size = max_pool_size_mb * 1024 * 1024\n",
        "        self.pools: Dict[Tuple[tuple, np.dtype], List[np.ndarray]] = {}\n",
        "        self.current_usage = 0; self.stats = PoolStats(); self._lock = threading.Lock()\n",
        "    def get_array(self, shape: tuple, dtype: np.dtype = np.float32, zero_fill: bool = True) -> np.ndarray:\n",
        "        key = (shape, dtype)\n",
        "        with self._lock:\n",
        "            if key in self.pools and self.pools[key]:\n",
        "                arr = self.pools[key].pop(); self.stats.hits += 1\n",
        "                if zero_fill: arr.fill(0)\n",
        "                return arr\n",
        "            else:\n",
        "                arr = np.empty(shape, dtype=dtype);\n",
        "                if zero_fill: arr.fill(0)\n",
        "                self.stats.misses += 1; self.stats.total_allocations += 1\n",
        "                return arr\n",
        "    def return_array(self, arr: np.ndarray) -> None:\n",
        "        if arr is None: return\n",
        "        key = (arr.shape, arr.dtype); array_size = arr.nbytes\n",
        "        with self._lock:\n",
        "            if self.current_usage + array_size <= self.max_pool_size:\n",
        "                if key not in self.pools: self.pools[key] = []\n",
        "                arr.fill(0); self.pools[key].append(arr); self.current_usage += array_size\n",
        "                self.stats.peak_usage_mb = max(self.stats.peak_usage_mb, self.current_usage / (1024 * 1024))\n",
        "_global_pool = ArrayPool()\n",
        "def get_pooled_array(shape: tuple, dtype: np.dtype = np.float32, zero_fill: bool = True) -> np.ndarray:\n",
        "    return _global_pool.get_array(shape, dtype, zero_fill)\n",
        "def return_pooled_array(arr: np.ndarray) -> None:\n",
        "    _global_pool.return_array(arr)\n",
        "\n",
        "# --- 2. Optimized Whitener (from training_coordinator_optimized.py) ---\n",
        "class OptimizedWhitener:\n",
        "    \"\"\"Memory-efficient online whitener\"\"\"\n",
        "    def __init__(self, dim: int, eps: float = 1e-6, momentum: float = 0.01):\n",
        "        self.dim = dim; self.eps = np.float32(eps); self.momentum = np.float32(momentum)\n",
        "        self.mu = np.zeros(dim, dtype=np.float32); self.var = np.ones(dim, dtype=np.float32)\n",
        "    def transform(self, x: np.ndarray) -> np.ndarray:\n",
        "        if x.dtype != np.float32: x = x.astype(np.float32)\n",
        "        _temp_diff = get_pooled_array((self.dim,), dtype=np.float32)\n",
        "        _temp_result = get_pooled_array((self.dim,), dtype=np.float32)\n",
        "        self.mu *= (1.0 - self.momentum); self.mu += self.momentum * x\n",
        "        np.subtract(x, self.mu, out=_temp_diff)\n",
        "        np.multiply(_temp_diff, _temp_diff, out=_temp_result)\n",
        "        self.var *= (1.0 - self.momentum); self.var += self.momentum * _temp_result\n",
        "        np.sqrt(self.var + self.eps, out=_temp_result)\n",
        "        np.divide(_temp_diff, _temp_result, out=_temp_result)\n",
        "        result_copy = _temp_result.copy()\n",
        "        return_pooled_array(_temp_diff); return_pooled_array(_temp_result)\n",
        "        return result_copy\n",
        "    def state_dict(self) -> Dict: return {\"mu\": self.mu, \"var\": self.var}\n",
        "    def load_state_dict(self, state: Dict): self.mu = state[\"mu\"]; self.var = state[\"var\"]\n",
        "\n",
        "# --- 3. GoEmotions Labels (The Curriculum) ---\n",
        "GOEMOTIONS_LABELS = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
        "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
        "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
        "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
        "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "\n",
        "# --- 4. \"Ears\": FeatureGenerator (GoEmotions-Aware) ---\n",
        "class FeatureGenerator:\n",
        "    \"\"\"Builds the 419-dim feature vector\"\"\"\n",
        "    def __init__(self, sbert_model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.SBERT_DIM = 384\n",
        "        self.SINE_LENGTH = 32\n",
        "        self.EXTRA_FEATURES = 3\n",
        "        self.TOTAL_FEATURES = self.SBERT_DIM + self.SINE_LENGTH + self.EXTRA_FEATURES # 419\n",
        "        print(f\"CREATED: FeatureGenerator (Aura 6.0), Features: {self.TOTAL_FEATURES}\")\n",
        "        self.sbert_model = SentenceTransformer(sbert_model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # Educate the feature generator on the *real* emotion labels\n",
        "        self.label_params = self._generate_default_params(GOEMOTIONS_LABELS)\n",
        "        self.tokenizer = self.sbert_model.tokenizer\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.whitener = OptimizedWhitener(dim=self.TOTAL_FEATURES)\n",
        "\n",
        "    def _generate_default_params(self, labels: List[str]) -> Dict[str, Dict]:\n",
        "        base_freq = 1.5; base_phase = 0.5; params = {}\n",
        "        for idx, label in enumerate(labels):\n",
        "            params[label] = {\"freq\": base_freq + 0.1 * idx, \"amp\": 0.7, \"phase\": base_phase + 0.2 * idx}\n",
        "        return params\n",
        "\n",
        "    def build_features(self, record: Dict[str, Any], sbert_vec: np.ndarray) -> np.ndarray:\n",
        "        prim = record.get(\"plutchik\", {}).get(\"primary\", \"neutral\")\n",
        "        if isinstance(prim, list): prim = prim[0] if prim else \"neutral\"\n",
        "        inten = float(record.get(\"plutchik\", {}).get(\"intensity\", 1.0))\n",
        "        cfg = self.label_params.get(prim, {\"freq\": 1.0, \"amp\": 0.0, \"phase\": 0.0}) # Default to flat line\n",
        "        t = np.linspace(0, 2*np.pi, self.SINE_LENGTH, dtype=np.float32)\n",
        "        emb = (cfg[\"amp\"] * inten * np.sin(cfg[\"freq\"] * t + cfg[\"phase\"])).astype(np.float32)\n",
        "        text = record.get(\"text\", \"\")\n",
        "        extras = np.array([\n",
        "            len(text) / 100.0,\n",
        "            int(\"!\" in text),\n",
        "            int(record.get(\"tone\", \"none\") in {\"euphoric\", \"tense\", \"somber\", \"peaceful\", \"amazed\"})\n",
        "        ], dtype=np.float32)\n",
        "        return np.concatenate([emb, extras, sbert_vec]).astype(np.float32)\n",
        "\n",
        "    def generate_for_query(self, query: str) -> (np.ndarray, List[int]):\n",
        "        record = {\n",
        "            \"text\": query,\n",
        "            \"plutchik\": {\"primary\": \"neutral\", \"intensity\": 0.5},\n",
        "            \"intent\": \"statement\", \"tone\": \"none\"\n",
        "        }\n",
        "        sbert_vec = self.sbert_model.encode(query, normalize_embeddings=True)\n",
        "        x_raw = self.build_features(record, sbert_vec)\n",
        "        x_whitened = self.whitener.transform(x_raw)\n",
        "        token_ids = self.tokenizer.encode(query, add_special_tokens=False)\n",
        "        return x_whitened, token_ids\n",
        "\n",
        "# --- 5. \"Mouth\": ResponseGenerator (Unsloth, Async) ---\n",
        "class ResponseGenerator:\n",
        "    def __init__(self, model_name: str = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"):\n",
        "        print(f\"CREATED: ResponseGenerator (Mouth) using Unsloth on '{model_name}'\")\n",
        "        max_seq_length = 2048; dtype = None; load_in_4bit = True\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = model_name, max_seq_length = max_seq_length,\n",
        "            dtype = dtype, load_in_4bit = load_in_4bit,\n",
        "        )\n",
        "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        print(\"   -> Unsloth Llama 3.1 model loaded successfully.\")\n",
        "    def _build_prompt(self, user_query: str, brain_state: dict) -> str:\n",
        "        cns_state = brain_state.get('cns_state', 'ALERT').name\n",
        "        stress = brain_state.get('stress_level', 0.0)\n",
        "        persona = f\"You are Aura, a bio-neural AI. Your current internal state is {cns_state}.\"\n",
        "        if cns_state == 'HYPERVIGILANT' or stress > 1.0:\n",
        "            persona += f\" You are feeling a high-stress level ({stress:.2f}). Your response should be direct and acknowledge the tension.\"\n",
        "        else: persona += \" You are calm and helpful.\"\n",
        "        messages = [{\"role\": \"system\", \"content\": persona}, {\"role\": \"user\", \"content\": user_query}]\n",
        "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    async def generate_response(self, user_query: str, brain_state: dict) -> str:\n",
        "        print(f\"\\n--- üëÑ ResponseGenerator (Unsloth) ---\"); print(f\"Generating response. Brain state: {brain_state}\")\n",
        "        prompt = self._build_prompt(user_query, brain_state)\n",
        "        inputs = self.tokenizer([prompt], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(\"cuda\")\n",
        "        terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "        outputs = await asyncio.to_thread(\n",
        "            self.model.generate, **inputs, max_new_tokens=150, eos_token_id=terminators,\n",
        "            do_sample=True, temperature=0.7, top_p=0.9,\n",
        "        )\n",
        "        response_text = self.tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[-1]:], skip_special_tokens=True)[0]\n",
        "        print(f\"Llama 3.1 (Unsloth) Output: {response_text}\"); return response_text\n",
        "\n",
        "# --- 6. SpikingAttention (k-WTA) (from snn_nlms_moe.py) ---\n",
        "@dataclass\n",
        "class SpikingAttention:\n",
        "    \"\"\"\"\"\"\n",
        "    decay: float = 0.7; theta: float = 1.0; k_winners: int = 5\n",
        "    gain_up: float = 1.5; gain_down: float = 0.6\n",
        "    def compute_gains(self, token_seq: List[int], vocab_size: int) -> Optional[np.ndarray]:\n",
        "        if not token_seq: return None\n",
        "        v: Dict[int, float] = {}; spikes: Dict[int, int] = {}\n",
        "        for j in token_seq:\n",
        "            vj = self.decay * v.get(j, 0.0) + 1.0\n",
        "            if vj >= self.theta: spikes[j] = spikes.get(j, 0) + 1; vj -= self.theta\n",
        "            v[j] = vj\n",
        "        ranked = sorted(spikes.items(), key=lambda kv: (-kv[1], -v.get(kv[0], 0.0)))\n",
        "        winners = set(j for j,_ in ranked[:max(1, self.k_winners)])\n",
        "        gains = np.ones(vocab_size, dtype=np.float64)\n",
        "        seen = set(spikes.keys()) | set(v.keys())\n",
        "        for j in seen:\n",
        "            if 0 <= j < vocab_size: gains[j] = self.gain_up if j in winners else self.gain_down\n",
        "        return gains\n",
        "\n",
        "# --- 7. \"Expert Neuron\": NLMSHead (from snn_nlms_moe.py) ---\n",
        "class NLMSHead:\n",
        "    \"\"\"This is the 'Expert'. It holds the weights and performs learning.\"\"\"\n",
        "    def __init__(self, n_features: int, n_outputs: int, vocab_size: int,\n",
        "                 attention_config: Dict, mu: float = 0.1):\n",
        "        self.n_features = n_features; self.n_outputs = n_outputs; self.vocab_size = vocab_size\n",
        "        self.attention_config = attention_config; self.mu = mu; self._lock = asyncio.Lock()\n",
        "        self.w = np.zeros((self.n_features, self.n_outputs), dtype=np.float64)\n",
        "        self.b = np.zeros(self.n_outputs, dtype=np.float64)\n",
        "        self.spiking_attention = SpikingAttention(**self.attention_config)\n",
        "        self.last_error = np.zeros(self.n_outputs, dtype=np.float64)\n",
        "        self.last_output = np.zeros(self.n_outputs, dtype=np.float64)\n",
        "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
        "         x = np.asarray(x, dtype=np.float64).reshape(-1); return (x @ self.w) + self.b\n",
        "    async def load_weights(self, w_path: str, b_path: str):\n",
        "        \"\"\"NEW: Load pre-trained weights from .npy files\"\"\"\n",
        "        async with self._lock:\n",
        "            try:\n",
        "                self.w = np.load(w_path); self.b = np.load(b_path)\n",
        "                print(f\"   -> Successfully loaded weights: {w_path} (W:{self.w.shape}, b:{self.b.shape})\")\n",
        "            except Exception as e:\n",
        "                print(f\"   -> WARNING: Failed to load weights from {w_path}. Starting from zero. Error: {e}\")\n",
        "    async def step(self, x: np.ndarray, y_true: np.ndarray | float, token_ids: List[int]) -> np.ndarray:\n",
        "        async with self._lock:\n",
        "            x = np.asarray(x, dtype=np.float64).reshape(-1)\n",
        "            y_hat = self.predict(x); self.last_output = y_hat\n",
        "            y_true_vec = (np.array([y_true]) if np.isscalar(y_true)\n",
        "                          else np.asarray(y_true, dtype=np.float64).reshape(-1))\n",
        "            e = y_true_vec - y_hat; self.last_error = e\n",
        "            attention_gains = self.spiking_attention.compute_gains(token_ids, self.vocab_size)\n",
        "            avg_gain = 1.0\n",
        "            if attention_gains is not None and token_ids:\n",
        "                gains_for_seq = [attention_gains[token] for token in token_ids if 0 <= token < self.vocab_size]\n",
        "                if gains_for_seq: avg_gain = np.mean(gains_for_seq)\n",
        "            modulated_mu = self.mu * avg_gain\n",
        "            x_norm_sq = 1e-8 + float(x @ x)\n",
        "            grad = (x / x_norm_sq)[:, None] * e[None, :]\n",
        "            self.w += modulated_mu * grad; self.b += modulated_mu * e\n",
        "            return y_hat\n",
        "    def state_dict(self) -> Dict: return {\"w\": self.w, \"b\": self.b}\n",
        "    def load_state_dict(self, state: Dict): self.w = state[\"w\"]; self.b = state[\"b\"]\n",
        "\n",
        "# --- 8. CNS & ThalamicRouter (Educated) ---\n",
        "class ConsciousnessLevel(Enum):\n",
        "    DEEP_SLEEP = 0; ASLEEP = 1; ALERT = 2; FOCUSED = 3; HYPERVIGILANT = 4\n",
        "class CentralNervousSystem:\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self):\n",
        "        self.consciousness_level = ConsciousnessLevel.ALERT\n",
        "        self.stress_level = 0.0 # This is our 'cortisol'\n",
        "        print(\"CREATED: CentralNervousSystem (CNS)\")\n",
        "    def set_consciousness(self, level: ConsciousnessLevel):\n",
        "        if self.consciousness_level != level:\n",
        "            self.consciousness_level = level; print(f\"CNS: Consciousness set to {level.name}\")\n",
        "    def update_stress(self, error: float):\n",
        "        new_stress = abs(error) * 1.5\n",
        "        self.stress_level = (self.stress_level * 0.5) + (new_stress * 0.5)\n",
        "        self.stress_level = max(0.0, self.stress_level - 0.1)\n",
        "        if self.stress_level > 1.0:\n",
        "            self.set_consciousness(ConsciousnessLevel.HYPERVIGILANT)\n",
        "        else:\n",
        "            self.set_consciousness(ConsciousnessLevel.ALERT)\n",
        "\n",
        "class ThalamicRouter:\n",
        "    \"\"\"\n",
        "    This router is now \"educated\" with the GoEmotions label map.\n",
        "    It provides target signals (y_true) for the experts.\n",
        "    \"\"\"\n",
        "    def __init__(self, label_maps: Dict):\n",
        "        # --- THIS IS THE FIX ---\n",
        "        self.label_maps = label_maps  # <-- ADD THIS LINE\n",
        "        # --- END FIX ---\n",
        "\n",
        "        self.emotion_map = label_maps.get('emotion', {})\n",
        "        self.intent_map = label_maps.get('intent', {})\n",
        "        print(f\"CREATED: ThalamicRouter (Educated with {len(self.emotion_map)} emotions)\")\n",
        "\n",
        "    def get_target_signals(self, query: str) -> dict:\n",
        "        query = query.lower()\n",
        "        signals = {\n",
        "            'emotion': self.emotion_map.get('neutral', 0.0),\n",
        "            'intent': self.intent_map.get('statement', 0.0)\n",
        "        }\n",
        "        # Keyword-based mapping to the *real* GoEmotions labels\n",
        "        if 'scared' in query or 'fear' in query or 'anxious' in query or 'nervous' in query:\n",
        "            signals['emotion'] = self.emotion_map.get('fear', self.emotion_map.get('nervousness', 0.0))\n",
        "        elif 'happy' in query or 'joy' in query or 'excited' in query:\n",
        "            signals['emotion'] = self.emotion_map.get('joy', self.emotion_map.get('excitement', 0.0))\n",
        "        elif 'angry' in query or 'annoyed' in query:\n",
        "            signals['emotion'] = self.emotion_map.get('anger', self.emotion_map.get('annoyance', 0.0))\n",
        "        elif 'sad' in query or 'grief' in query:\n",
        "            signals['emotion'] = self.emotion_map.get('sadness', self.emotion_map.get('grief', 0.0))\n",
        "\n",
        "        if '?' in query:\n",
        "            signals['intent'] = self.intent_map.get('question', 0.0)\n",
        "        elif '!' in query:\n",
        "            signals['intent'] = self.intent_map.get('exclamation', 0.0)\n",
        "\n",
        "        return signals\n",
        "\n",
        "# --- 9. The Final IBNN (Aura 6.0 - Pre-Trained) ---\n",
        "class IntegratedBioNeuralNetwork:\n",
        "    \"\"\"\n",
        "    This is Aura 6.0. It learns from a \"School\" (offline training)\n",
        "    and then fine-tunes in real-time (online learning).\n",
        "    \"\"\"\n",
        "    def __init__(self, llm_model_name: str = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"):\n",
        "        print(\"--- üß† Initializing Aura 6.0 (Pre-Trained) ---\")\n",
        "        self.feature_gen = FeatureGenerator()\n",
        "        self.response_gen = ResponseGenerator(llm_model_name)\n",
        "        self.cns = CentralNervousSystem()\n",
        "        self.feature_dim = self.feature_gen.TOTAL_FEATURES\n",
        "        self.vocab_size = self.feature_gen.vocab_size\n",
        "        self.experts: Dict[str, NLMSHead] = {}\n",
        "        self.router = None # Will be set after education\n",
        "        self.education_dir = \"/content/drive/MyDrive/aura_education_weights\" # Save to Drive\n",
        "        os.makedirs(self.education_dir, exist_ok=True)\n",
        "        print(\"--- Aura 6.0 Brain Initialized (Awaiting Education) ---\")\n",
        "\n",
        "    def _add_expert(self, name: str, n_outputs: int, attention_config: dict):\n",
        "        head = NLMSHead(\n",
        "            n_features=self.feature_dim,\n",
        "            n_outputs=n_outputs,\n",
        "            vocab_size=self.vocab_size,\n",
        "            attention_config=attention_config,\n",
        "            mu=0.05\n",
        "        )\n",
        "        self.experts[name] = head\n",
        "        print(f\"Added Expert: {name} (Output dim: {n_outputs})\")\n",
        "\n",
        "    async def educate_brain(self):\n",
        "        \"\"\"\n",
        "        Runs the 'School' pipeline to pre-train experts from GoEmotions.\n",
        "\n",
        "        \"\"\"\n",
        "        print(\"\\n--- üéì STARTING OFFLINE EDUCATION (GoEmotions) ---\")\n",
        "\n",
        "        # 1. Load GoEmotions dataset\n",
        "        print(\"Loading 'goemotions' dataset from Hugging Face...\")\n",
        "        dataset = load_dataset(\"google-research-datasets/go_emotions\", \"raw\")['train']\n",
        "        # Let's use a subset for speed in Colab\n",
        "        dataset = dataset.shuffle(seed=42).select(range(5000)) # 5000 samples\n",
        "\n",
        "        all_texts = dataset['text']\n",
        "\n",
        "        # 2. Precompute SBERT in a thread\n",
        "        print(f\"Pre-computing SBERT embeddings for {len(all_texts)} texts...\")\n",
        "        sbert_embeddings = await asyncio.to_thread(\n",
        "            self.feature_gen.sbert_model.encode,\n",
        "            all_texts, normalize_embeddings=True, batch_size=64\n",
        "        )\n",
        "\n",
        "        # 3. Create Label Maps\n",
        "        emotion_labels = GOEMOTIONS_LABELS\n",
        "        # Mock 'intent' labels as GoEmotions doesn't have them\n",
        "        intent_labels = [\"question\", \"statement\", \"exclamation\", \"request\", \"none\"]\n",
        "        label_maps = {\n",
        "            'emotion': {label: idx for idx, label in enumerate(emotion_labels)},\n",
        "            'intent': {label: idx for idx, label in enumerate(intent_labels)},\n",
        "        }\n",
        "\n",
        "        # 4. Build Features and Tensorize\n",
        "        print(\"Building features and tensorizing...\")\n",
        "        X_features, y_emotion, y_intent = [], [], []\n",
        "\n",
        "        for i, record in enumerate(dataset):\n",
        "            # Create a mock 'plutchik' record from the multi-label data\n",
        "            primary_emotion = 'neutral'\n",
        "            for label in emotion_labels:\n",
        "                if label != 'neutral' and record[label] == 1:\n",
        "                    primary_emotion = label\n",
        "                    break # Just take the first one\n",
        "\n",
        "            mock_record = {\n",
        "                \"text\": record['text'],\n",
        "                \"plutchik\": {\"primary\": primary_emotion, \"intensity\": 1.0},\n",
        "                \"intent\": \"question\" if \"?\" in record['text'] else \"statement\"\n",
        "            }\n",
        "\n",
        "            X_features.append(self.feature_gen.build_features(mock_record, sbert_embeddings[i]))\n",
        "            y_emotion.append(label_maps['emotion'].get(primary_emotion, 27)) # 27 is 'neutral'\n",
        "            y_intent.append(label_maps['intent'].get(mock_record['intent'], 1)) # 1 is 'statement'\n",
        "\n",
        "        X_train = torch.tensor(np.stack(X_features), dtype=torch.float32)\n",
        "        y_emotion_train = torch.tensor(y_emotion, dtype=torch.long)\n",
        "        y_intent_train = torch.tensor(y_intent, dtype=torch.long)\n",
        "\n",
        "        # --- 5. Train \"Emotion\" Expert ---\n",
        "        print(f\"Training Emotion expert ({self.feature_gen.TOTAL_FEATURES} -> {len(emotion_labels)} classes)...\")\n",
        "        emotion_model = LinearTorchModel(self.feature_gen.TOTAL_FEATURES, len(emotion_labels))\n",
        "        optimizer = optim.AdamW(emotion_model.parameters(), lr=5e-3)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        for epoch in range(15): # 15 epochs\n",
        "            optimizer.zero_grad(); loss = criterion(emotion_model(X_train), y_emotion_train)\n",
        "            loss.backward(); optimizer.step()\n",
        "        preds = emotion_model(X_train).argmax(dim=1); acc = (preds == y_emotion_train).float().mean().item()\n",
        "        print(f\"  -> Emotion training complete. Final Acc: {acc:.2f}\")\n",
        "\n",
        "        # --- 6. Train \"Intent\" Expert ---\n",
        "        print(f\"Training Intent expert ({self.feature_gen.TOTAL_FEATURES} -> {len(intent_labels)} classes)...\")\n",
        "        intent_model = LinearTorchModel(self.feature_gen.TOTAL_FEATURES, len(intent_labels))\n",
        "        optimizer = optim.AdamW(intent_model.parameters(), lr=5e-3)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        for epoch in range(10): # 10 epochs\n",
        "            optimizer.zero_grad(); loss = criterion(intent_model(X_train), y_intent_train)\n",
        "            loss.backward(); optimizer.step()\n",
        "        preds = intent_model(X_train).argmax(dim=1); acc = (preds == y_intent_train).float().mean().item()\n",
        "        print(f\"  -> Intent training complete. Final Acc: {acc:.2f}\")\n",
        "\n",
        "        # --- 7. Export Weights to Google Drive ---\n",
        "        print(f\"Exporting educated weights to {self.education_dir}...\")\n",
        "        export_linear_weights(emotion_model, self.education_dir, \"emotion_classifier\")\n",
        "        export_linear_weights(intent_model, self.education_dir, \"intent_classifier\")\n",
        "\n",
        "        # --- 8. \"Mind Upload\" - Load weights into NLMSHead Experts ---\n",
        "        await self.load_education(label_maps, weights_dir=self.education_dir)\n",
        "\n",
        "        print(\"--- üéì OFFLINE EDUCATION COMPLETE ---\")\n",
        "\n",
        "    async def load_education(self, label_maps: Dict, weights_dir: str = \".\"):\n",
        "        \"\"\"\n",
        "        Create experts based on label maps and load pre-trained weights.\n",
        "\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- üìö Loading Education from {weights_dir} ---\")\n",
        "        self.experts.clear() # Clear any old experts\n",
        "\n",
        "        if 'emotion' in label_maps:\n",
        "            self._add_expert(\n",
        "                name='emotion',\n",
        "                n_outputs=len(label_maps['emotion']),\n",
        "                attention_config={'decay': 0.6, 'k_winners': 4, 'gain_up': 2.0, 'gain_down': 0.4}\n",
        "            )\n",
        "            await self.experts['emotion'].load_weights(\n",
        "                w_path=os.path.join(weights_dir, \"emotion_classifier_W.npy\"),\n",
        "                b_path=os.path.join(weights_dir, \"emotion_classifier_b.npy\")\n",
        "            )\n",
        "        if 'intent' in label_maps:\n",
        "            self._add_expert(\n",
        "                name='intent',\n",
        "                n_outputs=len(label_maps['intent']),\n",
        "                attention_config={'decay': 0.7, 'k_winners': 5, 'gain_up': 1.5, 'gain_down': 0.7}\n",
        "            )\n",
        "            await self.experts['intent'].load_weights(\n",
        "                w_path=os.path.join(weights_dir, \"intent_classifier_W.npy\"),\n",
        "                b_path=os.path.join(weights_dir, \"intent_classifier_b.npy\")\n",
        "            )\n",
        "        self.router = ThalamicRouter(label_maps)\n",
        "\n",
        "    async def process_query(self, query: str) -> str:\n",
        "        print(f\"\\n--- ‚òÄÔ∏è PROCESSING QUERY (Aura 6.0): '{query}' ---\")\n",
        "        if not self.router:\n",
        "            return \"I am uneducated. Please run `await aura.educate_brain()`.\"\n",
        "\n",
        "        target_signals = self.router.get_target_signals(query)\n",
        "        cns_level = self.cns.consciousness_level\n",
        "        x_whitened, token_ids = self.feature_gen.generate_for_query(query)\n",
        "\n",
        "        tasks = []\n",
        "        for name, expert in self.experts.items():\n",
        "            num_classes = expert.n_outputs\n",
        "            target_index = target_signals.get(name, 0)\n",
        "            y_true = np.zeros(num_classes)\n",
        "            y_true[target_index] = 1.0 # One-hot target\n",
        "            tasks.append(expert.step(x_whitened, y_true, token_ids))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "        emotion_error_vec = self.experts['emotion'].last_error\n",
        "        stress = np.mean(np.abs(emotion_error_vec))\n",
        "        self.cns.update_stress(stress)\n",
        "\n",
        "        emotion_pred_index = self.experts['emotion'].last_output.argmax()\n",
        "        emotion_pred_label = GOEMOTIONS_LABELS[emotion_pred_index]\n",
        "        print(f\"Emotion Prediction: {emotion_pred_label} (Error/Stress: {stress:.3f})\")\n",
        "\n",
        "        final_brain_state = {\n",
        "            'cns_state': self.cns.consciousness_level,\n",
        "            'stress_level': self.cns.stress_level,\n",
        "        }\n",
        "        response = await self.response_gen.generate_response(query, final_brain_state)\n",
        "        self.last_run_final_stress = self.cns.stress_level\n",
        "        return response\n",
        "\n",
        "    async def save_brain(self, path: str):\n",
        "        \"\"\"Saves the *learned* (fine-tuned) brain state.\"\"\"\n",
        "        print(f\"\\n--- üíæ Saving brain state to {path} ---\")\n",
        "        state_dict = {}\n",
        "        for name, expert in self.experts.items():\n",
        "            async with expert._lock:\n",
        "                state_dict[f\"expert_{name}_w\"] = expert.w\n",
        "                state_dict[f\"expert_{name}_b\"] = expert.b\n",
        "        state_dict[\"whitener_mu\"] = self.feature_gen.whitener.mu\n",
        "        state_dict[\"whitener_var\"] = self.feature_gen.whitener.var\n",
        "        await asyncio.to_thread(np.savez_compressed, path, **state_dict)\n",
        "        print(\"--- üíæ Save complete ---\")\n",
        "\n",
        "    async def load_brain(self, path: str):\n",
        "        \"\"\"Loads a saved brain state from a .npz file.\"\"\"\n",
        "        print(f\"\\n--- üß† Loading brain state from {path} ---\")\n",
        "        try:\n",
        "            data = np.load(path)\n",
        "            for name, expert in self.experts.items():\n",
        "                async with expert._lock:\n",
        "                    if f\"expert_{name}_w\" in data:\n",
        "                        expert.w = data[f\"expert_{name}_w\"]\n",
        "                        expert.b = data[f\"expert_{name}_b\"]\n",
        "                        print(f\"   -> Loaded weights for expert: {name}\")\n",
        "            if \"whitener_mu\" in data:\n",
        "                self.feature_gen.whitener.mu = data[\"whitener_mu\"]\n",
        "                self.feature_gen.whitener.var = data[\"whitener_var\"]\n",
        "            print(\"--- üß† Load complete ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"--- ‚ùå ERROR: Failed to load brain state. {e} ---\")\n",
        "\n",
        "# --- 11. PyTorch Models (for the \"School\") ---\n",
        "class LinearTorchModel(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, input_dim: int, num_classes: int):\n",
        "        super().__init__(); self.fc = nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, x): return self.fc(x)\n",
        "\n",
        "def export_linear_weights(model: LinearTorchModel, output_dir: str, task_name: str):\n",
        "    \"\"\"\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    W = model.fc.weight.detach().cpu().numpy().T\n",
        "    b = model.fc.bias.detach().cpu().numpy()\n",
        "    np.save(os.path.join(output_dir, f\"{task_name}_W.npy\"), W)\n",
        "    np.save(os.path.join(output_dir, f\"{task_name}_b.npy\"), b)\n",
        "    print(f\"Exported {task_name} weights: W{W.shape}, b{b.shape}\")"
      ],
      "metadata": {
        "id": "q0WT5RvETJUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- This is the test cell for Aura 6.0 ---\n",
        "# It must be run in a cell *after* Cell 50\n",
        "import os\n",
        "# --- 1. Mount Google Drive ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    SAVE_DIR = \"/content/drive/MyDrive/aura_education_weights\"\n",
        "    print(f\"Google Drive mounted. Will save/load brain from: {SAVE_DIR}\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab. Saving to local directory './aura_brain_state_v6'\")\n",
        "    SAVE_DIR = \"./aura_brain_state_v6\"\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BRAIN_STATE_FILE = os.path.join(SAVE_DIR, \"aura_6_memory.npz\")\n",
        "LABEL_MAPS_FILE = os.path.join(SAVE_DIR, \"aura_6_labels.json\")\n",
        "\n",
        "\n",
        "# --- 2. The Main Async Test Loop ---\n",
        "async def main_test_loop():\n",
        "\n",
        "    # --- 3. INITIALIZE AND EDUCATE (Day 1) ---\n",
        "    print(\"--- üß† Initializing Aura 6.0 (Loading LLMs with Unsloth...) ---\")\n",
        "    aura_day_1 = IntegratedBioNeuralNetwork(\n",
        "        llm_model_name=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    )\n",
        "    print(\"--- Aura LLM Components Loaded ---\")\n",
        "\n",
        "    # Run the \"School\"\n",
        "    await aura_day_1.educate_brain()\n",
        "\n",
        "    # Save the label maps\n",
        "    label_maps = aura_day_1.router.label_maps\n",
        "    with open(LABEL_MAPS_FILE, 'w') as f:\n",
        "        json.dump(label_maps, f)\n",
        "\n",
        "    # --- 4. TEST 1: (PRE-TRAINED, \"scared\") ---\n",
        "    print(\"\\n\\n--- üó£Ô∏è TEST 1: Educated Brain, 'Scared' Query ---\")\n",
        "    response_1 = await aura_day_1.process_query(\"I am feeling very scared\")\n",
        "    print(f\"\\nFINAL AURA RESPONSE (Day 1): {response_1}\")\n",
        "    day_1_stress = aura_day_1.last_run_final_stress\n",
        "\n",
        "    # --- 5. TEST 2: (PRE-TRAINED, \"happy\") ---\n",
        "    print(\"\\n\\n--- üó£Ô∏è TEST 2: Educated Brain, 'Happy' Query ---\")\n",
        "    response_2 = await aura_day_1.process_query(\"I am so happy and full of joy!\")\n",
        "    print(f\"\\nFINAL AURA RESPONSE (Day 1, Happy): {response_2}\")\n",
        "\n",
        "    # --- 6. SAVE THE BRAIN ---\n",
        "    # At this point, the brain has been fine-tuned on \"scared\" and \"happy\"\n",
        "    await aura_day_1.save_brain(BRAIN_STATE_FILE)\n",
        "\n",
        "    # --- 7. LOAD THE BRAIN (Simulate a restart) ---\n",
        "    print(\"\\n\\n--- üß† Initializing NEW Aura 6.0 (Blank Brain) ---\")\n",
        "    aura_day_2 = IntegratedBioNeuralNetwork(\n",
        "        llm_model_name=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    )\n",
        "\n",
        "    # Give it the *same* education so its experts are the right shape\n",
        "    print(\"--- Loading 'School' curriculum (label maps) ---\")\n",
        "    with open(LABEL_MAPS_FILE, 'r') as f:\n",
        "        loaded_label_maps = json.load(f)\n",
        "\n",
        "    # Load the *weights*\n",
        "    await aura_day_2.load_education(label_maps=loaded_label_maps, weights_dir=SAVE_DIR)\n",
        "    # Load the *fine-tuned state*\n",
        "    await aura_day_2.load_brain(BRAIN_STATE_FILE)\n",
        "\n",
        "    # --- 8. TEST 3: (LOADED BRAIN, \"scared\") ---\n",
        "    print(\"\\n\\n--- üó£Ô∏è TEST 3: LOADED Brain, 'Scared' Query (Day 2) ---\")\n",
        "    response_3 = await aura_day_2.process_query(\"I am feeling very scared\")\n",
        "    print(f\"\\nFINAL AURA RESPONSE (Day 2): {response_3}\")\n",
        "    day_2_stress = aura_day_2.last_run_final_stress\n",
        "\n",
        "    # --- 9. VERIFICATION ---\n",
        "    print(\"\\n\\n--- üî¨ PERSISTENCE VERIFICATION ---\")\n",
        "    print(f\"Day 1 Final Stress (on 'scared'): {day_1_stress:.4f}\")\n",
        "    print(f\"Day 2 Final Stress (on 'scared'): {day_2_stress:.4f}\")\n",
        "\n",
        "    if day_2_stress < (day_1_stress * 1.1): # Allow for small floating point differences\n",
        "        print(\"‚úÖ SUCCESS: The loaded brain's stress response was persistent. Learning was saved and restored!\")\n",
        "    else:\n",
        "        print(\"‚ùå FAILURE: The loaded brain's stress was different. Persistence failed.\")\n",
        "\n",
        "# --- Run the async main loop ---\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "asyncio.run(main_test_loop())"
      ],
      "metadata": {
        "id": "IurdrqdCTdc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}