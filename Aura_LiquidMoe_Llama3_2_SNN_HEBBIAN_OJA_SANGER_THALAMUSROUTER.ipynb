{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1EaRNSajynPMZN22RAtLh53agkMfY2eIp",
      "authorship_tag": "ABX9TyNePS2LeFCBkLrYLD9IYj/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auralmn/aura_liquidmoe_snn_llama-3.2-3B_Notebooks/blob/main/Aura_LiquidMoe_Llama3_2_SNN_HEBBIAN_OJA_SANGER_THALAMUSROUTER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHUUirjJHFC"
      },
      "outputs": [],
      "source": [
        "# AURA LIQUID MOE (With trained thalamus router)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AURA LIQUIDMOE SNN + LLAMA 3.2 3B UNSLOTH LLM (L4-T4)\n"
      ],
      "metadata": {
        "id": "a1unRLWYJfZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install dependencies"
      ],
      "metadata": {
        "id": "BX6it6TaJqmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip uninstall wandb\n",
        "!pip install transformers torch sentence-transformers accelerate numpy\n",
        "!pip install huggingface_hub\n",
        "# 1. Install Unsloth\n",
        "!pip install \"unsloth[colab-new]\"\n",
        "\n",
        "!pip install huggingface_hub datasets\n",
        "\n",
        "\n",
        "# 3. Authenticate (from Step 13)\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful!\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab or 'HF_TOKEN' secret not found.\")\n",
        "# --- Log into Hugging Face ---\n",
        "# This is the best way to do it in Colab.\n",
        "# 1. Go to your Colab notebook.\n",
        "# 2. Click the \"Key\" icon (Secrets) in the left-hand sidebar.\n",
        "# 3. Create a new secret named \"HF_TOKEN\".\n",
        "# 4. Paste your Hugging Face Pro token (it starts with \"hf_\") as the value.\n",
        "# 5. Make sure \"Notebook access\" is toggled ON.\n",
        "\n",
        "# This code will then automatically find and use your secret.\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful!\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab or 'HF_TOKEN' secret not found.\")\n",
        "    # You can paste your token manually if prompted\n",
        "    # login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_f9MqN2TKQz6",
        "outputId": "96bf8a3d-276a-4ba5-c6e7-8b59d563588e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: wandb 0.22.3\n",
            "Uninstalling wandb-0.22.3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/wandb\n",
            "    /usr/local/bin/wb\n",
            "    /usr/local/lib/python3.12/dist-packages/package_readme.md\n",
            "    /usr/local/lib/python3.12/dist-packages/wandb-0.22.3.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/wandb/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled wandb-0.22.3\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.10.5)\n",
            "Collecting unsloth[colab-new]\n",
            "  Downloading unsloth-2025.11.3-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.11.4 (from unsloth[colab-new])\n",
            "  Downloading unsloth_zoo-2025.11.4-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.45.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (25.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (5.9.5)\n",
            "Collecting tyro (from unsloth[colab-new])\n",
            "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (5.29.5)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth[colab-new])\n",
            "  Downloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth[colab-new])\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (3.4.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.2.1)\n",
            "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth[colab-new])\n",
            "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (1.11.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.17.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.35.2)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (4.57.1)\n",
            "Collecting trl!=0.19.0,<=0.23.0,>=0.18.2 (from unsloth[colab-new])\n",
            "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth[colab-new]) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth[colab-new]) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (3.20.0)\n",
            "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new])\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth[colab-new]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth[colab-new]) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth[colab-new]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth[colab-new]) (0.22.1)\n",
            "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.4->unsloth[colab-new])\n",
            "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.11.4->unsloth[colab-new])\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth[colab-new]) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.11.4->unsloth[colab-new])\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting torch>=2.4.0 (from unsloth[colab-new])\n",
            "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cufft-cu12==11.3.0.4->torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth[colab-new])\n",
            "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth[colab-new]) (8.7.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth[colab-new])\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth[colab-new]) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth[colab-new]) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new])\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth[colab-new]) (4.4.4)\n",
            "\u001b[33mWARNING: unsloth 2025.11.3 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth[colab-new]) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth[colab-new]) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth[colab-new]) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth[colab-new]) (1.3.1)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.11.4-py3-none-any.whl (283 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m146.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.11.3-py3-none-any.whl (353 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.0/353.0 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchao, triton, shtab, pyarrow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, tyro, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvshmem-cu12\n",
            "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
            "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
            "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP CLEAN AURA INSTANCE"
      ],
      "metadata": {
        "id": "2kGteWhnKuZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum"
      ],
      "metadata": {
        "id": "EHsp-1lMT8oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- All Imports ---\n",
        "import uuid\n",
        "import enum\n",
        "from enum import Enum\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import itertools\n",
        "import torch\n",
        "from transformers import AutoTokenizer, logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "from typing import List, Dict, Optional, Union, Any, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from abc import ABC, abstractmethod\n",
        "from numpy import ndarray\n",
        "import asyncio\n",
        "import io\n",
        "import csv\n",
        "import threading # For Memory Pool\n",
        "import time\n",
        "from datasets import load_dataset # For GoEmotions\n",
        "\n",
        "# --- PyTorch Imports (for Pre-Training) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Suppress transformer warnings\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# --- 0. Colab/HF Login ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"Hugging Face login successful!\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"HF_TOKEN secret not found or not in Colab. Ensure you are logged in.\")\n",
        "\n",
        "# --- 1. Memory Pool (from memory_pool.py) ---\n",
        "@dataclass\n",
        "class PoolStats:\n",
        "    hits: int = 0; misses: int = 0; total_allocations: int = 0; peak_usage_mb: float = 0.0\n",
        "class ArrayPool:\n",
        "    \"\"\"High-performance array pool\"\"\"\n",
        "    def __init__(self, max_pool_size_mb: int = 512):\n",
        "        self.max_pool_size = max_pool_size_mb * 1024 * 1024\n",
        "        self.pools: Dict[Tuple[tuple, np.dtype], List[np.ndarray]] = {}\n",
        "        self.current_usage = 0; self.stats = PoolStats(); self._lock = threading.Lock()\n",
        "    def get_array(self, shape: tuple, dtype: np.dtype = np.float32, zero_fill: bool = True) -> np.ndarray:\n",
        "        key = (shape, dtype)\n",
        "        with self._lock:\n",
        "            if key in self.pools and self.pools[key]:\n",
        "                arr = self.pools[key].pop(); self.stats.hits += 1\n",
        "                if zero_fill: arr.fill(0)\n",
        "                return arr\n",
        "            else:\n",
        "                arr = np.empty(shape, dtype=dtype);\n",
        "                if zero_fill: arr.fill(0)\n",
        "                self.stats.misses += 1; self.stats.total_allocations += 1\n",
        "                return arr\n",
        "    def return_array(self, arr: np.ndarray) -> None:\n",
        "        if arr is None: return\n",
        "        key = (arr.shape, arr.dtype); array_size = arr.nbytes\n",
        "        with self._lock:\n",
        "            if self.current_usage + array_size <= self.max_pool_size:\n",
        "                if key not in self.pools: self.pools[key] = []\n",
        "                arr.fill(0); self.pools[key].append(arr); self.current_usage += array_size\n",
        "                self.stats.peak_usage_mb = max(self.stats.peak_usage_mb, self.current_usage / (1024 * 1024))\n",
        "_global_pool = ArrayPool()\n",
        "def get_pooled_array(shape: tuple, dtype: np.dtype = np.float32, zero_fill: bool = True) -> np.ndarray:\n",
        "    return _global_pool.get_array(shape, dtype, zero_fill)\n",
        "def return_pooled_array(arr: np.ndarray) -> None:\n",
        "    _global_pool.return_array(arr)\n",
        "\n",
        "# --- 2. Optimized Whitener (from training_coordinator_optimized.py) ---\n",
        "class OptimizedWhitener:\n",
        "    \"\"\"Memory-efficient online whitener\"\"\"\n",
        "    def __init__(self, dim: int, eps: float = 1e-6, momentum: float = 0.01):\n",
        "        self.dim = dim; self.eps = np.float32(eps); self.momentum = np.float32(momentum)\n",
        "        self.mu = np.zeros(dim, dtype=np.float32); self.var = np.ones(dim, dtype=np.float32)\n",
        "    def transform(self, x: np.ndarray) -> np.ndarray:\n",
        "        if x.dtype != np.float32: x = x.astype(np.float32)\n",
        "        _temp_diff = get_pooled_array((self.dim,), dtype=np.float32)\n",
        "        _temp_result = get_pooled_array((self.dim,), dtype=np.float32)\n",
        "        self.mu *= (1.0 - self.momentum); self.mu += self.momentum * x\n",
        "        np.subtract(x, self.mu, out=_temp_diff)\n",
        "        np.multiply(_temp_diff, _temp_diff, out=_temp_result)\n",
        "        self.var *= (1.0 - self.momentum); self.var += self.momentum * _temp_result\n",
        "        np.sqrt(self.var + self.eps, out=_temp_result)\n",
        "        np.divide(_temp_diff, _temp_result, out=_temp_result)\n",
        "        result_copy = _temp_result.copy()\n",
        "        return_pooled_array(_temp_diff); return_pooled_array(_temp_result)\n",
        "        return result_copy\n",
        "    def state_dict(self) -> Dict: return {\"mu\": self.mu, \"var\": self.var}\n",
        "    def load_state_dict(self, state: Dict): self.mu = state[\"mu\"]; self.var = state[\"var\"]\n",
        "\n",
        "# --- 3. GoEmotions Labels (The Curriculum) ---\n",
        "GOEMOTIONS_LABELS = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
        "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
        "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
        "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
        "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "INTENT_LABELS = [\"question\", \"statement\", \"exclamation\", \"request\", \"none\"] # Mock intents\n",
        "\n",
        "# --- 4. \"Ears\": FeatureGenerator (GoEmotions-Aware) ---\n",
        "class FeatureGenerator:\n",
        "    \"\"\"Builds the 419-dim feature vector\"\"\"\n",
        "    def __init__(self, sbert_model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.SBERT_DIM = 384\n",
        "        self.SINE_LENGTH = 32\n",
        "        self.EXTRA_FEATURES = 3\n",
        "        self.TOTAL_FEATURES = self.SBERT_DIM + self.SINE_LENGTH + self.EXTRA_FEATURES # 419\n",
        "        print(f\"CREATED: FeatureGenerator (Aura 6.0), Features: {self.TOTAL_FEATURES}\")\n",
        "        self.sbert_model = SentenceTransformer(sbert_model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.label_params = self._generate_default_params(GOEMOTIONS_LABELS)\n",
        "        self.tokenizer = self.sbert_model.tokenizer\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.whitener = OptimizedWhitener(dim=self.TOTAL_FEATURES)\n",
        "\n",
        "    def _generate_default_params(self, labels: List[str]) -> Dict[str, Dict]:\n",
        "        base_freq = 1.5; base_phase = 0.5; params = {}\n",
        "        for idx, label in enumerate(labels):\n",
        "            params[label] = {\"freq\": base_freq + 0.1 * idx, \"amp\": 0.7, \"phase\": base_phase + 0.2 * idx}\n",
        "        return params\n",
        "\n",
        "    def build_features(self, record: Dict[str, Any], sbert_vec: np.ndarray) -> np.ndarray:\n",
        "        prim = record.get(\"plutchik\", {}).get(\"primary\", \"neutral\")\n",
        "        if isinstance(prim, list): prim = prim[0] if prim else \"neutral\"\n",
        "        inten = float(record.get(\"plutchik\", {}).get(\"intensity\", 1.0))\n",
        "        cfg = self.label_params.get(prim, {\"freq\": 1.0, \"amp\": 0.0, \"phase\": 0.0})\n",
        "        t = np.linspace(0, 2*np.pi, self.SINE_LENGTH, dtype=np.float32)\n",
        "        emb = (cfg[\"amp\"] * inten * np.sin(cfg[\"freq\"] * t + cfg[\"phase\"])).astype(np.float32)\n",
        "        text = record.get(\"text\", \"\")\n",
        "        extras = np.array([\n",
        "            len(text) / 100.0,\n",
        "            int(\"!\" in text),\n",
        "            int(record.get(\"tone\", \"none\") in {\"euphoric\", \"tense\", \"somber\", \"peaceful\", \"amazed\"})\n",
        "        ], dtype=np.float32)\n",
        "        return np.concatenate([emb, extras, sbert_vec]).astype(np.float32)\n",
        "\n",
        "    def generate_for_query(self, query: str) -> (np.ndarray, List[int]):\n",
        "        record = {\n",
        "            \"text\": query,\n",
        "            \"plutchik\": {\"primary\": \"neutral\", \"intensity\": 0.5},\n",
        "            \"intent\": \"statement\", \"tone\": \"none\"\n",
        "        }\n",
        "        sbert_vec = self.sbert_model.encode(query, normalize_embeddings=True)\n",
        "        x_raw = self.build_features(record, sbert_vec)\n",
        "        x_whitened = self.whitener.transform(x_raw)\n",
        "        token_ids = self.tokenizer.encode(query, add_special_tokens=False)\n",
        "        return x_whitened, token_ids\n",
        "\n",
        "# --- 5. \"Mouth\": ResponseGenerator (Unsloth, Async) ---\n",
        "class ResponseGenerator:\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
        "        print(f\"CREATED: ResponseGenerator (Mouth) using Unsloth on '{model_name}'\")\n",
        "        max_seq_length = 2048; dtype = None; load_in_4bit = True\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = model_name, max_seq_length = max_seq_length,\n",
        "            dtype = dtype, load_in_4bit = load_in_4bit,\n",
        "        )\n",
        "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        print(\"   -> Unsloth Llama 3.2 model loaded successfully.\")\n",
        "    def _build_prompt(self, user_query: str, brain_state: dict) -> str:\n",
        "        cns_state = brain_state.get('cns_state', 'ALERT').name\n",
        "        stress = brain_state.get('stress_level', 0.0)\n",
        "        persona = f\"You are Aura, a bio-neural AI. Your current internal state is {cns_state}.\"\n",
        "        if cns_state == 'HYPERVIGILANT' or stress > 1.0:\n",
        "            persona += f\" You are feeling a high-stress level ({stress:.2f}). Your response should be direct and acknowledge the tension.\"\n",
        "        else: persona += \" You are calm and helpful.\"\n",
        "        messages = [{\"role\": \"system\", \"content\": persona}, {\"role\": \"user\", \"content\": user_query}]\n",
        "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    async def generate_response(self, user_query: str, brain_state: dict) -> str:\n",
        "        print(f\"\\n--- 👄 ResponseGenerator (Unsloth) ---\"); print(f\"Generating response. Brain state: {brain_state}\")\n",
        "        prompt = self._build_prompt(user_query, brain_state)\n",
        "        inputs = self.tokenizer([prompt], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(\"cuda\")\n",
        "        terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "        outputs = await asyncio.to_thread(\n",
        "            self.model.generate, **inputs, max_new_tokens=150, eos_token_id=terminators,\n",
        "            do_sample=True, temperature=0.7, top_p=0.9,\n",
        "        )\n",
        "        response_text = self.tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[-1]:], skip_special_tokens=True)[0]\n",
        "        print(f\"Llama 3.2 (Unsloth) Output: {response_text}\"); return response_text\n",
        "\n",
        "# --- 6. SpikingAttention (k-WTA) (from snn_nlms_moe.py) ---\n",
        "@dataclass\n",
        "class SpikingAttention:\n",
        "    \"\"\"\"\"\"\n",
        "    decay: float = 0.7; theta: float = 1.0; k_winners: int = 5\n",
        "    gain_up: float = 1.5; gain_down: float = 0.6\n",
        "    def compute_gains(self, token_seq: List[int], vocab_size: int) -> Optional[np.ndarray]:\n",
        "        if not token_seq: return None\n",
        "        v: Dict[int, float] = {}; spikes: Dict[int, int] = {}\n",
        "        for j in token_seq:\n",
        "            vj = self.decay * v.get(j, 0.0) + 1.0\n",
        "            if vj >= self.theta: spikes[j] = spikes.get(j, 0) + 1; vj -= self.theta\n",
        "            v[j] = vj\n",
        "        ranked = sorted(spikes.items(), key=lambda kv: (-kv[1], -v.get(kv[0], 0.0)))\n",
        "        winners = set(j for j,_ in ranked[:max(1, self.k_winners)])\n",
        "        gains = np.ones(vocab_size, dtype=np.float64)\n",
        "        seen = set(spikes.keys()) | set(v.keys())\n",
        "        for j in seen:\n",
        "            if 0 <= j < vocab_size: gains[j] = self.gain_up if j in winners else self.gain_down\n",
        "        return gains\n",
        "\n",
        "# --- 7. \"Expert Neuron\": NLMSHead (from snn_nlms_moe.py) ---\n",
        "class NLMSHead:\n",
        "    \"\"\"This is the 'Expert'. It holds the weights and performs learning.\"\"\"\n",
        "    def __init__(self, n_features: int, n_outputs: int, vocab_size: int,\n",
        "                 attention_config: Dict, mu: float = 0.1):\n",
        "        self.n_features = n_features; self.n_outputs = n_outputs; self.vocab_size = vocab_size\n",
        "        self.attention_config = attention_config; self.mu = mu; self._lock = asyncio.Lock()\n",
        "        self.w = np.zeros((self.n_features, self.n_outputs), dtype=np.float64)\n",
        "        self.b = np.zeros(self.n_outputs, dtype=np.float64)\n",
        "        self.spiking_attention = SpikingAttention(**self.attention_config)\n",
        "        self.last_error = np.zeros(self.n_outputs, dtype=np.float64)\n",
        "        self.last_output = np.zeros(self.n_outputs, dtype=np.float64)\n",
        "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
        "         x = np.asarray(x, dtype=np.float64).reshape(-1); return (x @ self.w) + self.b\n",
        "    async def load_weights(self, w_path: str, b_path: str):\n",
        "        \"\"\"NEW: Load pre-trained weights from .npy files\"\"\"\n",
        "        async with self._lock:\n",
        "            try:\n",
        "                self.w = np.load(w_path); self.b = np.load(b_path)\n",
        "                print(f\"   -> Successfully loaded weights: {w_path} (W:{self.w.shape}, b:{self.b.shape})\")\n",
        "            except Exception as e:\n",
        "                print(f\"   -> WARNING: Failed to load weights from {w_path}. Starting from zero. Error: {e}\")\n",
        "    async def step(self, x: np.ndarray, y_true: np.ndarray | float, token_ids: List[int]) -> np.ndarray:\n",
        "        async with self._lock:\n",
        "            x = np.asarray(x, dtype=np.float64).reshape(-1)\n",
        "            y_hat = self.predict(x); self.last_output = y_hat\n",
        "            y_true_vec = (np.array([y_true]) if np.isscalar(y_true)\n",
        "                          else np.asarray(y_true, dtype=np.float64).reshape(-1))\n",
        "            e = y_true_vec - y_hat; self.last_error = e\n",
        "            attention_gains = self.spiking_attention.compute_gains(token_ids, self.vocab_size)\n",
        "            avg_gain = 1.0\n",
        "            if attention_gains is not None and token_ids:\n",
        "                gains_for_seq = [attention_gains[token] for token in token_ids if 0 <= token < self.vocab_size]\n",
        "                if gains_for_seq: avg_gain = np.mean(gains_for_seq)\n",
        "            modulated_mu = self.mu * avg_gain\n",
        "            x_norm_sq = 1e-8 + float(x @ x)\n",
        "            grad = (x / x_norm_sq)[:, None] * e[None, :]\n",
        "            self.w += modulated_mu * grad; self.b += modulated_mu * e\n",
        "            return y_hat\n",
        "    def state_dict(self) -> Dict: return {\"w\": self.w, \"b\": self.b}\n",
        "    def load_state_dict(self, state: Dict): self.w = state[\"w\"]; self.b = state[\"b\"]\n",
        "\n",
        "# --- 8. PyTorch Models (for the \"School\") ---\n",
        "class LinearTorchModel(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, input_dim: int, num_classes: int):\n",
        "        super().__init__(); self.fc = nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, x): return self.fc(x)\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, input_dim: int, num_emotions: int, num_intents: int):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(0.1))\n",
        "        self.emotion_head = nn.Linear(256, num_emotions)\n",
        "        self.intent_head = nn.Linear(256, num_intents)\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "        return self.emotion_head(h), self.intent_head(h)\n",
        "    def predict(self, x):\n",
        "        h = self.shared(x)\n",
        "        return self.emotion_head(h).argmax(dim=1), self.intent_head(h).argmax(dim=1)\n",
        "\n",
        "def multitask_loss(emotion_out, emotion_y, intent_out, intent_y, weights=(1.0, 0.7)):\n",
        "    \"\"\"\"\"\"\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    return (weights[0] * ce(emotion_out, emotion_y) +\n",
        "            weights[1] * ce(intent_out, intent_y))\n",
        "\n",
        "def export_linear_weights(model: LinearTorchModel, output_dir: str, task_name: str):\n",
        "    \"\"\"\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    W = model.fc.weight.detach().cpu().numpy().T\n",
        "    b = model.fc.bias.detach().cpu().numpy()\n",
        "    np.save(os.path.join(output_dir, f\"{task_name}_W.npy\"), W)\n",
        "    np.save(os.path.join(output_dir, f\"{task_name}_b.npy\"), b)\n",
        "    print(f\"Exported {task_name} weights: W{W.shape}, b{b.shape}\")\n",
        "\n",
        "# --- 9. CNS & ThalamicRouter (Trained) ---\n",
        "class ConsciousnessLevel(Enum):\n",
        "    DEEP_SLEEP = 0; ASLEEP = 1; ALERT = 2; FOCUSED = 3; HYPERVIGILANT = 4\n",
        "class CentralNervousSystem:\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self):\n",
        "        self.consciousness_level = ConsciousnessLevel.ALERT\n",
        "        self.stress_level = 0.0 # This is our 'cortisol'\n",
        "        print(\"CREATED: CentralNervousSystem (CNS)\")\n",
        "    def set_consciousness(self, level: ConsciousnessLevel):\n",
        "        if self.consciousness_level != level:\n",
        "            self.consciousness_level = level; print(f\"CNS: Consciousness set to {level.name}\")\n",
        "    def update_stress(self, error: float):\n",
        "        new_stress = abs(error) * 1.5\n",
        "        self.stress_level = (self.stress_level * 0.5) + (new_stress * 0.5)\n",
        "        self.stress_level = max(0.0, self.stress_level - 0.1)\n",
        "        if self.stress_level > 1.0:\n",
        "            self.set_consciousness(ConsciousnessLevel.HYPERVIGILANT)\n",
        "        else:\n",
        "            self.set_consciousness(ConsciousnessLevel.ALERT)\n",
        "\n",
        "class TrainedThalamicRouter:\n",
        "    \"\"\"\n",
        "    This is the new Thalamus. It uses a trained PyTorch MultiTaskHead\n",
        "    to PREDICT the 'y_true' (desired output) for the other experts.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_gen: FeatureGenerator, label_maps: Dict, model_path: str):\n",
        "        self.feature_gen = feature_gen\n",
        "        self.label_maps = label_maps\n",
        "        self.emotion_map = label_maps.get('emotion', {})\n",
        "        self.intent_map = label_maps.get('intent', {})\n",
        "\n",
        "        # Load the trained PyTorch model\n",
        "        self.model = MultiTaskHead(\n",
        "            feature_gen.TOTAL_FEATURES,\n",
        "            len(self.emotion_map),\n",
        "            len(self.intent_map)\n",
        "        )\n",
        "        try:\n",
        "            self.model.load_state_dict(torch.load(model_path))\n",
        "            self.model.eval() # Set to inference mode\n",
        "            print(f\"CREATED: TrainedThalamicRouter (Loaded from {model_path})\")\n",
        "        except Exception as e:\n",
        "            print(f\"--- WARNING: Failed to load ThalamicRouter model. Using mock. Error: {e} ---\")\n",
        "            self.model = None # Fallback to mock\n",
        "\n",
        "    def get_target_signals(self, query: str) -> dict:\n",
        "        \"\"\"\n",
        "        Runs a *real* inference pass to predict emotion and intent.\n",
        "        \"\"\"\n",
        "        if not self.model: # Mock fallback\n",
        "            return {'emotion': 0, 'intent': 0}\n",
        "\n",
        "        # 1. Generate the same feature vector the model was trained on\n",
        "        # We pass a \"neutral\" CNS state for this baseline prediction\n",
        "        x_whitened, _ = self.feature_gen.generate_for_query(query)\n",
        "        x_tensor = torch.tensor(x_whitened, dtype=torch.float32).unsqueeze(0) # Add batch dim\n",
        "\n",
        "        # 2. Predict with the PyTorch model\n",
        "        with torch.no_grad():\n",
        "            emotion_logits, intent_logits = self.model(x_tensor)\n",
        "\n",
        "        emotion_index = emotion_logits.argmax(dim=1).item()\n",
        "        intent_index = intent_logits.argmax(dim=1).item()\n",
        "\n",
        "        return {\n",
        "            'emotion': emotion_index,\n",
        "            'intent': intent_index\n",
        "        }\n",
        "\n",
        "# --- 10. The Final IBNN (Aura 6.0 - Pre-Trained) ---\n",
        "class IntegratedBioNeuralNetwork:\n",
        "    \"\"\"\n",
        "    This is Aura 6.0. It learns from a \"School\" (offline training)\n",
        "    and then fine-tunes in real-time (online learning).\n",
        "    \"\"\"\n",
        "    def __init__(self, llm_model_name: str = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"):\n",
        "        print(\"--- 🧠 Initializing Aura 6.0 (Pre-Trained) ---\")\n",
        "        self.feature_gen = FeatureGenerator()\n",
        "        self.response_gen = ResponseGenerator(llm_model_name)\n",
        "        self.cns = CentralNervousSystem()\n",
        "\n",
        "        self.feature_dim = self.feature_gen.TOTAL_FEATURES\n",
        "        self.vocab_size = self.feature_gen.vocab_size\n",
        "        self.experts: Dict[str, NLMSHead] = {}\n",
        "        self.router: Optional[TrainedThalamicRouter] = None # Will be set after education\n",
        "        self.education_dir = \"/content/drive/MyDrive/aura_education_v6\"\n",
        "        os.makedirs(self.education_dir, exist_ok=True)\n",
        "        print(\"--- Aura 6.0 Brain Initialized (Awaiting Education) ---\")\n",
        "\n",
        "    def _add_expert(self, name: str, n_outputs: int, attention_config: dict):\n",
        "        head = NLMSHead(\n",
        "            n_features=self.feature_dim,\n",
        "            n_outputs=n_outputs,\n",
        "            vocab_size=self.vocab_size,\n",
        "            attention_config=attention_config,\n",
        "            mu=0.05\n",
        "        )\n",
        "        self.experts[name] = head\n",
        "        print(f\"Added Expert: {name} (Output dim: {n_outputs})\")\n",
        "\n",
        "    async def educate_brain(self):\n",
        "        \"\"\"\n",
        "        Runs the 'School' pipeline to pre-train all components.\n",
        "\n",
        "        \"\"\"\n",
        "        print(\"\\n--- 🎓 STARTING OFFLINE EDUCATION (GoEmotions) ---\")\n",
        "\n",
        "        # 1. Load GoEmotions dataset\n",
        "        print(\"Loading 'goemotions' dataset from Hugging Face...\")\n",
        "        dataset = load_dataset(\"google-research-datasets/go_emotions\", \"raw\")['train']\n",
        "        dataset = dataset.shuffle(seed=42).select(range(5000)) # 5000 samples\n",
        "        all_texts = dataset['text']\n",
        "\n",
        "        # 2. Precompute SBERT\n",
        "        print(f\"Pre-computing SBERT embeddings for {len(all_texts)} texts...\")\n",
        "        sbert_embeddings = await asyncio.to_thread(\n",
        "            self.feature_gen.sbert_model.encode,\n",
        "            all_texts, normalize_embeddings=True, batch_size=64\n",
        "        )\n",
        "\n",
        "        # 3. Create Label Maps\n",
        "        emotion_labels = GOEMOTIONS_LABELS\n",
        "        intent_labels = INTENT_LABELS\n",
        "        label_maps = {\n",
        "            'emotion': {label: idx for idx, label in enumerate(emotion_labels)},\n",
        "            'intent': {label: idx for idx, label in enumerate(intent_labels)},\n",
        "        }\n",
        "\n",
        "        # 4. Build Features and Tensorize\n",
        "        print(\"Building features and tensorizing...\")\n",
        "        X_features, y_emotion, y_intent = [], [], []\n",
        "        for i, record in enumerate(dataset):\n",
        "            primary_emotion = 'neutral'\n",
        "            for label in emotion_labels:\n",
        "                if label != 'neutral' and record[label] == 1:\n",
        "                    primary_emotion = label; break\n",
        "            mock_record = {\n",
        "                \"text\": record['text'],\n",
        "                \"plutchik\": {\"primary\": primary_emotion, \"intensity\": 1.0},\n",
        "                \"intent\": \"question\" if \"?\" in record['text'] else \"statement\"\n",
        "            }\n",
        "            X_features.append(self.feature_gen.build_features(mock_record, sbert_embeddings[i]))\n",
        "            y_emotion.append(label_maps['emotion'].get(primary_emotion, 27))\n",
        "            y_intent.append(label_maps['intent'].get(mock_record['intent'], 1))\n",
        "\n",
        "        X_train = torch.tensor(np.stack(X_features), dtype=torch.float32)\n",
        "        y_emotion_train = torch.tensor(y_emotion, dtype=torch.long)\n",
        "        y_intent_train = torch.tensor(y_intent, dtype=torch.long)\n",
        "\n",
        "        # --- 5. Train the MultiTaskHead (This is the new Thalamus) ---\n",
        "        print(f\"Training MultiTask ThalamicRouter...\")\n",
        "        thalamus_model = MultiTaskHead(\n",
        "            self.feature_gen.TOTAL_FEATURES,\n",
        "            len(label_maps['emotion']),\n",
        "            len(label_maps['intent'])\n",
        "        )\n",
        "        optimizer = optim.AdamW(thalamus_model.parameters(), lr=5e-3)\n",
        "        for epoch in range(15): # 15 epochs\n",
        "            optimizer.zero_grad()\n",
        "            emotion_out, intent_out = thalamus_model(X_train)\n",
        "            loss = multitask_loss(emotion_out, y_emotion_train, intent_out, y_intent_train)\n",
        "            loss.backward(); optimizer.step()\n",
        "\n",
        "        # --- 6. Save the Thalamus Model ---\n",
        "        thalamus_path = os.path.join(self.education_dir, \"thalamic_router.pt\")\n",
        "        torch.save(thalamus_model.state_dict(), thalamus_path)\n",
        "        print(f\"  -> ThalamicRouter trained and saved to {thalamus_path}\")\n",
        "\n",
        "        # --- 7. \"Mind Upload\" - Load the trained components ---\n",
        "        await self.load_education(label_maps, thalamus_path)\n",
        "\n",
        "        print(\"--- 🎓 OFFLINE EDUCATION COMPLETE ---\")\n",
        "\n",
        "    async def load_education(self, label_maps: Dict, thalamus_model_path: str):\n",
        "        \"\"\"\n",
        "        Create experts based on label maps and load pre-trained weights.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- 📚 Loading Education ---\")\n",
        "        self.experts.clear()\n",
        "\n",
        "        # 1. Create and Load the ThalamicRouter\n",
        "        self.router = TrainedThalamicRouter(self.feature_gen, label_maps, thalamus_model_path)\n",
        "\n",
        "        # 2. Create the Experts (untrained, they will learn in real-time)\n",
        "        if 'emotion' in label_maps:\n",
        "            self._add_expert(\n",
        "                name='emotion',\n",
        "                n_outputs=len(label_maps['emotion']),\n",
        "                attention_config={'decay': 0.6, 'k_winners': 4, 'gain_up': 2.0, 'gain_down': 0.4}\n",
        "            )\n",
        "        if 'intent' in label_maps:\n",
        "            self._add_expert(\n",
        "                name='intent',\n",
        "                n_outputs=len(label_maps['intent']),\n",
        "                attention_config={'decay': 0.7, 'k_winners': 5, 'gain_up': 1.5, 'gain_down': 0.7}\n",
        "            )\n",
        "\n",
        "    async def process_query(self, query: str) -> str:\n",
        "        print(f\"\\n--- ☀️ PROCESSING QUERY (Aura 6.0): '{query}' ---\")\n",
        "        if not self.router:\n",
        "            return \"I am uneducated. Please run `await aura.educate_brain()`.\"\n",
        "\n",
        "        # 1. Get Target Signals (The Thalamus *predicts* the emotion)\n",
        "        target_signals = self.router.get_target_signals(query)\n",
        "\n",
        "        # 2. Get Features (Senses)\n",
        "        cns_level = self.cns.consciousness_level\n",
        "        x_whitened, token_ids = self.feature_gen.generate_for_query(query)\n",
        "\n",
        "        # 3. Run all Experts Concurrently (Thinking & Real-time Learning)\n",
        "        tasks = []\n",
        "        for name, expert in self.experts.items():\n",
        "            num_classes = expert.n_outputs\n",
        "            target_index = target_signals.get(name, 0)\n",
        "            y_true = np.zeros(num_classes); y_true[target_index] = 1.0 # One-hot target\n",
        "            tasks.append(expert.step(x_whitened, y_true, token_ids))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "        # 4. Update CNS (Feeling)\n",
        "        emotion_error_vec = self.experts['emotion'].last_error\n",
        "        stress = np.mean(np.abs(emotion_error_vec))\n",
        "        self.cns.update_stress(stress)\n",
        "\n",
        "        emotion_pred_label = GOEMOTIONS_LABELS[target_signals['emotion']] # Get label from router\n",
        "        print(f\"Thalamus Predicted: '{emotion_pred_label}' (Error/Stress: {stress:.3f})\")\n",
        "\n",
        "        # 5. \"Mouth\": Generate Response\n",
        "        final_brain_state = {\n",
        "            'cns_state': self.cns.consciousness_level,\n",
        "            'stress_level': self.cns.stress_level,\n",
        "        }\n",
        "        response = await self.response_gen.generate_response(query, final_brain_state)\n",
        "        self.last_run_final_stress = self.cns.stress_level\n",
        "        return response\n",
        "\n",
        "    async def save_brain(self, path: str):\n",
        "        \"\"\"Saves the *learned* (fine-tuned) brain state.\"\"\"\n",
        "        print(f\"\\n--- 💾 Saving brain state to {path} ---\")\n",
        "        state_dict = {}\n",
        "        for name, expert in self.experts.items():\n",
        "            async with expert._lock:\n",
        "                state_dict[f\"expert_{name}_w\"] = expert.w\n",
        "                state_dict[f\"expert_{name}_b\"] = expert.b\n",
        "        state_dict[\"whitener_mu\"] = self.feature_gen.whitener.mu\n",
        "        state_dict[\"whitener_var\"] = self.feature_gen.whitener.var\n",
        "        await asyncio.to_thread(np.savez_compressed, path, **state_dict)\n",
        "        print(\"--- 💾 Save complete ---\")\n",
        "\n",
        "    async def load_brain(self, path: str):\n",
        "        \"\"\"Loads a saved brain state from a .npz file.\"\"\"\n",
        "        print(f\"\\n--- 🧠 Loading brain state from {path} ---\")\n",
        "        if not self.experts:\n",
        "            print(\"--- ❌ ERROR: Must run 'educate_brain' or 'load_education' first to create experts. ---\")\n",
        "            return\n",
        "        try:\n",
        "            data = np.load(path)\n",
        "            for name, expert in self.experts.items():\n",
        "                async with expert._lock:\n",
        "                    if f\"expert_{name}_w\" in data:\n",
        "                        expert.w = data[f\"expert_{name}_w\"]\n",
        "                        expert.b = data[f\"expert_{name}_b\"]\n",
        "                        print(f\"   -> Loaded weights for expert: {name}\")\n",
        "            if \"whitener_mu\" in data:\n",
        "                self.feature_gen.whitener.mu = data[\"whitener_mu\"]\n",
        "                self.feature_gen.whitener.var = data[\"whitener_var\"]\n",
        "            print(\"--- 🧠 Load complete ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"--- ❌ ERROR: Failed to load brain state. {e} ---\")"
      ],
      "metadata": {
        "id": "q0WT5RvETJUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- This is the test cell for Aura 6.0 ---\n",
        "# It must be run in a cell *after* Cell 51\n",
        "\n",
        "import enum\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    SAVE_DIR = \"/content/drive/MyDrive/aura_brain_state_v6_final\"\n",
        "    print(f\"Google Drive mounted. Will save/load brain from: {SAVE_DIR}\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab. Saving to local directory './aura_brain_state_v6_final'\")\n",
        "    SAVE_DIR = \"./aura_brain_state_v6_final\"\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BRAIN_STATE_FILE = os.path.join(SAVE_DIR, \"aura_6_memory.npz\")\n",
        "LABEL_MAPS_FILE = os.path.join(SAVE_DIR, \"aura_6_labels.json\")\n",
        "THALAMUS_MODEL_FILE = os.path.join(SAVE_DIR, \"thalamic_router.pt\")\n",
        "\n",
        "\n",
        "# --- 2. The Main Async Test Loop ---\n",
        "async def main_test_loop():\n",
        "\n",
        "    # --- 3. INITIALIZE AND EDUCATE (Day 1) ---\n",
        "    print(\"--- 🧠 Initializing Aura 6.0 (Loading LLMs with Unsloth...) ---\")\n",
        "    aura_day_1 = IntegratedBioNeuralNetwork(\n",
        "        llm_model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "    )\n",
        "    print(\"--- Aura LLM Components Loaded ---\")\n",
        "\n",
        "    # Run the \"School\" - This trains and saves the Thalamus\n",
        "    await aura_day_1.educate_brain()\n",
        "\n",
        "    # Save the label maps\n",
        "    label_maps = aura_day_1.router.label_maps\n",
        "    with open(LABEL_MAPS_FILE, 'w') as f:\n",
        "        json.dump(label_maps, f)\n",
        "\n",
        "    # --- 4. TEST 1: (PRE-TRAINED, \"scared\") ---\n",
        "    print(\"\\n\\n--- 🗣️ TEST 1: Educated Brain, 'Scared' Query ---\")\n",
        "    response_1 = await aura_day_1.process_query(\"I am feeling very scared\")\n",
        "    print(f\"\\nFINAL AURA RESPONSE (Day 1): {response_1}\")\n",
        "    day_1_stress = aura_day_1.last_run_final_stress\n",
        "\n",
        "    # --- 5. TEST 2: (PRE-TRAINED, \"happy\") ---\n",
        "    print(\"\\n\\n--- 🗣️ TEST 2: Educated Brain, 'Happy' Query ---\")\n",
        "    response_2 = await aura_day_1.process_query(\"I am so happy and full of joy!\")\n",
        "    print(f\"\\nFINAL AURA RESPONSE (Day 1, Happy): {response_2}\")\n",
        "\n",
        "    # --- 6. SAVE THE BRAIN ---\n",
        "    # At this point, the brain has been fine-tuned on \"scared\" and \"happy\"\n",
        "    await aura_day_1.save_brain(BRAIN_STATE_FILE)\n",
        "\n",
        "    # --- 7. LOAD THE BRAIN (Simulate a restart) ---\n",
        "    print(\"\\n\\n--- 🧠 Initializing NEW Aura 6.0 (Blank Brain) ---\")\n",
        "    aura_day_2 = IntegratedBioNeuralNetwork(\n",
        "        llm_model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "    )\n",
        "\n",
        "    print(\"--- Loading 'School' curriculum (label maps) ---\")\n",
        "    with open(LABEL_MAPS_FILE, 'r') as f:\n",
        "        loaded_label_maps = json.load(f)\n",
        "\n",
        "    # Load the *Trained Thalamus* and *create* blank experts\n",
        "    await aura_day_2.load_education(label_maps=loaded_label_maps, thalamus_model_path=THALAMUS_MODEL_FILE)\n",
        "\n",
        "    # Load the *fine-tuned expert state* from Day 1\n",
        "    await aura_day_2.load_brain(BRAIN_STATE_FILE)\n",
        "\n",
        "    # --- 8. TEST 3: (LOADED BRAIN, \"scared\") ---\n",
        "    print(\"\\n\\n--- 🗣️ TEST 3: LOADED Brain, 'Scared' Query (Day 2) ---\")\n",
        "    response_3 = await aura_day_2.process_query(\"I am feeling very scared\")\n",
        "    print(f\"\\nFINAL AURA RESPONSE (Day 2): {response_3}\")\n",
        "    day_2_stress = aura_day_2.last_run_final_stress\n",
        "\n",
        "    # --- 9. VERIFICATION ---\n",
        "    print(\"\\n\\n--- 🔬 PERSISTENCE VERIFICATION ---\")\n",
        "    print(f\"Day 1 Final Stress (on 'scared'): {day_1_stress:.4f}\")\n",
        "    print(f\"Day 2 Final Stress (on 'scared'): {day_2_stress:.4f}\")\n",
        "\n",
        "    if day_2_stress < (day_1_stress * 0.9):\n",
        "        print(\"✅ SUCCESS: The loaded brain's stress was significantly lower. Real-time learning was saved and restored!\")\n",
        "    elif day_2_stress < (day_1_stress * 1.1):\n",
        "        print(\"✅ SUCCESS (Mild): The loaded brain's stress was persistent. Learning is present.\")\n",
        "    else:\n",
        "        print(\"❌ FAILURE: The loaded brain's stress was different. Persistence failed.\")\n",
        "\n",
        "# --- Run the async main loop ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "asyncio.run(main_test_loop())"
      ],
      "metadata": {
        "id": "IurdrqdCTdc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}